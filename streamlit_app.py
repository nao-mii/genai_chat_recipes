import streamlit as st
from app.llm_provider import get_llm
from app.recipe_engine import load_local_recipes, filter_candidates, build_llm_messages
from app.prompts import SYSTEM_PROMPT, USER_GUIDANCE

# ===== Config e t√≠tulo
st.set_page_config(page_title="GenAI Recipe Chat", page_icon="üç≥", layout="centered")
st.title("üç≥ GenAI Recipe Chat")
st.caption("Assistente de receitas com IA ‚Äî PT-BR")

# ===== Prefer√™ncias (sidebar)
with st.sidebar:
    st.subheader("Prefer√™ncias")
    servings = st.number_input("Por√ß√µes", min_value=1, max_value=20, value=2)
    time_limit = st.number_input("Tempo m√°x. (min)", min_value=0, max_value=240, value=30)
    cuisine = st.text_input("Cozinha (opcional)", value="")
    dietary = st.multiselect("Restri√ß√µes", ["vegan", "vegetariano", "sem_gluten", "sem_lactose", "low_carb"])
    pantry_text = st.text_area("Ingredientes dispon√≠veis (separe por v√≠rgula)", "frango, lim√£o, alho, azeite")
    pantry = [p.strip() for p in pantry_text.split(",") if p.strip()]

    # === Painel de diagn√≥stico
    with st.expander("üîç Diagn√≥stico r√°pido"):
        import os, sys
        st.write("**Vers√µes**")
        try:
            import openai, httpx
            st.code(f"openai={openai.__version__} | httpx={httpx.__version__} | python={sys.version.split()[0]}")
        except Exception as e:
            st.warning(f"N√£o consegui ler vers√µes: {e}")

        st.write("**Chaves & ambiente**")
        # N√£o mostramos a chave por seguran√ßa; s√≥ um status
        api_key_present = bool(os.environ.get("OPENAI_API_KEY"))  # pode ou n√£o estar via env
        st.write(f"OPENAI_API_KEY (env) definido? {'‚úÖ' if api_key_present else '‚ùå'}")
        st.write("Se voc√™ usa settings.yml, a verifica√ß√£o de chave ser√° feita ao criar o cliente.")

# ===== Estado de conversa
if "history" not in st.session_state:
    st.session_state.history = []

# ===== Cria cliente LLM e carrega receitas
# get_llm() j√° valida a chave via settings.yml e retorna uma fun√ß√£o chat(...)
chat = get_llm()
local_recipes = load_local_recipes()

# ===== Caixa de chat
user_input = st.chat_input("O que voc√™ quer cozinhar hoje? (ex.: 'jantar r√°pido sem lactose')")

if user_input:
    # guarda hist√≥rico do usu√°rio
    st.session_state.history.append({"role": "user", "content": user_input})

    # seleciona candidatos do dataset local
    candidates = filter_candidates(
        recipes=local_recipes,
        pantry=pantry,
        dietary=dietary,
        time_limit=int(time_limit) if time_limit else None,
        cuisine=cuisine if cuisine else None
    )

    context = dict(
        pantry=", ".join(pantry),
        dietary=", ".join(dietary) or "nenhuma",
        time_limit=time_limit or "sem limite",
        servings=servings,
        cuisine=cuisine or "indiferente"
    )

    messages = build_llm_messages(SYSTEM_PROMPT, USER_GUIDANCE, context, candidates)
    # adiciona um pouco de mem√≥ria (sem exagero pra n√£o sair caro)
    messages += st.session_state.history[-5:]

    # === Chamada protegida ao LLM
    try:
        response = chat(messages)
    except Exception as e:
        # Se sua fun√ß√£o chat j√° trata exce√ß√µes e retorna string, isso provavelmente n√£o ser√° chamado.
        response = f"‚ö†Ô∏è Erro inesperado ao chamar o LLM: {e.__class__.__name__} ‚Äî {e}"

    st.session_state.history.append({"role": "assistant", "content": response})

# ===== Renderiza hist√≥rico
for m in st.session_state.history:
    with st.chat_message("user" if m["role"] == "user" else "assistant"):
        st.markdown(m["content"])